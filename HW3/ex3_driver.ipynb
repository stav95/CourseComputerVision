{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ex3-driver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5Ieisfun2l"
      },
      "source": [
        "**Name:** Stav Yosef\r\n",
        "\r\n",
        "**ID:** 316298876"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1CJTdh4uuha"
      },
      "source": [
        "**Colab:** [https://drive.google.com/file/d/1QpBrvWYaW58PTS5bgKhTRJ_Y-HqkfMVu/view?usp=sharing](https://drive.google.com/file/d/1QpBrvWYaW58PTS5bgKhTRJ_Y-HqkfMVu/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "7pCV4mnt7bVa"
      },
      "source": [
        "##  <span style=\"color:blue\">Exercise 3 - Driver file </span>\n",
        "## <span style=\"color:blue\">Computer Vision - Fall 2020\n",
        "\n",
        "\n",
        "**Lecturer:** Prof. Yael Moses, IDC\n",
        "\n",
        "**TA:** Eyal Friedman, IDC\n",
        "\n",
        "**Sybmission date: 11.1.2021**\n",
        "\n",
        "\n",
        "\n",
        "In this exercise you will practice working with videos, and simple segmentations.\n",
        "\n",
        "## Submission guidelines:\n",
        "\n",
        "1. Your zip file should include the following files only:\n",
        "    - ex2-driver.ipynb  **Or**  ex2-driver.py \n",
        "    - ex2_ID_ID.doc  **Or**  ex2_ID_ID.pdf\n",
        "2. The results you are asked to display and the open questions should be answered in a doc/pdf file. \n",
        "   (Don't add the python code to that file.)\n",
        "4. You may use any IDE  (e.g., Spyder, Jupyter Notebook, Pycharm, ect.).\n",
        "5. Name the file 'ex2_ID_ID.zip' and do **not** include any additional directories. \n",
        "6. Submit using *moodle*\n",
        "7. Submit on time!\n",
        "8. You can submit this assignment in pairs (no triples).\n",
        "\n",
        "## Read the following instructions carefully:\n",
        "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Do not submit your tests, unless requested.\n",
        "3. Use `python 3` and `numpy 1.18.5`. Changes of the configuration we provided are at your own risk. Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works.\n",
        "4. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden, unless been provided by us.\n",
        "4. Your code must run without errors. Note,  **code that fails to  run will not be graded.**\n",
        "5. Document your code properly.\n",
        "6. **Note:** you are given a set of videos, you are welcome to use them or any other videos. If they are too long, you can use only part of the frames. If they are too large, you can rescale them.\n",
        "7. In case there  are several possible variations for implementing the algorithms - make your own  choice, and give a short explanation.\n",
        "\n",
        "## Honor Code:\n",
        "The assignment is a basic tool for learning the material. You can probably find the solution on the Web. However, if  you do so, then you will not learn what you should learn from it. In addition, since we  grade  the assignment, using existing solutions will be considered dishonest.\n",
        "In particular, you are not allowed to copy or use any code that solves the task. \n",
        "You are more than welcome to talk with your friends, but you are not allowed to give your code or answers and you are not allowed to use their code or answers. \n",
        "Remember â€“ you take this course in order to learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4BswPmn7bVe"
      },
      "source": [
        "import cv2\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import numpy as np\n",
        "from scipy.linalg import null_space\n",
        "from scipy.signal import convolve2d\n",
        "from scipy.ndimage import gaussian_filter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmbtboYI7bVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4676896-d1b7-4954-e172-07b80b2941e0"
      },
      "source": [
        "import platform\n",
        "print(\"Python version: \", platform.python_version())\n",
        "print(\"Numpy version: \", np.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version:  3.6.9\n",
            "Numpy version:  1.19.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuzzQqYs7d2_"
      },
      "source": [
        "def get_dict_video_files() -> dict:\r\n",
        "    return {\r\n",
        "        'OneLeaveShopReenter2cor':\r\n",
        "            {\r\n",
        "                'input': \"OneLeaveShopReenter2cor.mpg\",\r\n",
        "                'output_a_1': \"OneLeaveShopReenter2cor_a_1.avi\",\r\n",
        "                'output_a_2': \"OneLeaveShopReenter2cor_a_2.avi\"\r\n",
        "            },\r\n",
        "        'SLIDE':\r\n",
        "            {\r\n",
        "                'input': \"SLIDE.avi\",\r\n",
        "                'output_a_1': \"SLIDE_a_1.avi\",\r\n",
        "                'output_a_2': \"SLIDE_a_2.avi\",\r\n",
        "            },\r\n",
        "        'marple7':\r\n",
        "            {\r\n",
        "                'input': \"marple7.avi\",\r\n",
        "                'output_a_1': \"marple7_a_1.avi\",\r\n",
        "                'output_a_2': \"marple7_a_2.avi\",\r\n",
        "            },\r\n",
        "        'horses':\r\n",
        "            {\r\n",
        "                'input': \"horses.avi\",\r\n",
        "                'output_a_1': \"horses_a_1.avi\",\r\n",
        "                'output_a_2': \"horses_a_2.avi\",\r\n",
        "            }\r\n",
        "        ,\r\n",
        "        'cars5':\r\n",
        "            {\r\n",
        "                'input': \"cars5.avi\",\r\n",
        "                'output_a_1': \"cars5_a_1.avi\",\r\n",
        "                'output_a_2': \"cars5_a_2.avi\",\r\n",
        "            }\r\n",
        "\r\n",
        "    }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09eV8seu7k-V"
      },
      "source": [
        "def save_output(v_foreground: np.ndarray, out_filename_video: str):\r\n",
        "    y = v_foreground.shape[2]\r\n",
        "    x = v_foreground.shape[1]\r\n",
        "\r\n",
        "    out = cv2.VideoWriter(out_filename_video, cv2.VideoWriter_fourcc(*\"MJPG\"), 30, (y, x))\r\n",
        "\r\n",
        "    for i in range(len(v_foreground)):\r\n",
        "        out.write(v_foreground[i].astype('uint8'))\r\n",
        "\r\n",
        "    out.release()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "Oz3GZNDq7bVf"
      },
      "source": [
        "## Section A: Change Detection\n",
        "\n",
        "**A1. Simple change detection**\n",
        "\n",
        "Compute a simple change detection algorithm. Use as background the median of a set of k1 frames, and update it every k2 frames. Your algorithm should work on color images. Think how to merge the different channels (colors). You can assume that the camera is static. The output is a video where the pixels of the  foreground objects consists of the original frame, and the other pixels are black. \n",
        "\n",
        "*Input:* name_file, k1, k2, and any other parameter you would like to add\\\n",
        "name_file: a name of a video file\\\n",
        "k1: the number of frames for computing the median\\\n",
        "k2: the number of frames between two updates of the background\n",
        "\n",
        "*output:*  v_foreground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMudu7bz7nTl"
      },
      "source": [
        "def median_change_dection(name_file: str, k1: int = 10, k2: int = 10) -> np.ndarray:\r\n",
        "    \"\"\"\r\n",
        "    :param name_file: Video filename (path)\r\n",
        "    :param k1: The number of frames for computing the median.\r\n",
        "    :param k2: The number of frames between two updates of the background.\r\n",
        "    :return: Reconstructed video with background and foreground objects.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    frames = None  # K1 frames\r\n",
        "\r\n",
        "    idx_change = 0  # Counting from 0 to K1, when reaching to k1 restarting to 0 (updating the frames list).\r\n",
        "    update_counter = 0  # Couting from 0 to K2, when reaching to k2 initialize background update and restarting to 0.\r\n",
        "    frames_counter = 0  # Total frames counted in total.\r\n",
        "\r\n",
        "    threshold = 0.25  # Threshold for the background range\r\n",
        "    lower_bound_bg = None  # Lower boundray for all pixels\r\n",
        "    higher_bound_bg = None  # Lower boundray for all pixels\r\n",
        "\r\n",
        "    cap = cv2.VideoCapture(name_file)\r\n",
        "\r\n",
        "    rows, cols = -1, -1\r\n",
        "\r\n",
        "    v_foreground = []\r\n",
        "\r\n",
        "    while cap.isOpened():\r\n",
        "        ret, frame = cap.read()\r\n",
        "        if ret:\r\n",
        "            if frames_counter == 0:\r\n",
        "                # Initializing video resolution.\r\n",
        "\r\n",
        "                rows = frame.shape[0]\r\n",
        "                cols = frame.shape[1]\r\n",
        "\r\n",
        "                # K1 frames, each by the size (rows, cols). each pixel is RGB. (CV2 -> BGR)\r\n",
        "                frames = np.zeros((k1, rows, cols, 3))\r\n",
        "\r\n",
        "            frames_counter += 1\r\n",
        "\r\n",
        "            frames[idx_change] = frame\r\n",
        "            idx_change += 1\r\n",
        "\r\n",
        "            if k1 == idx_change:\r\n",
        "                # If true restart the counting and start to override the first frame.\r\n",
        "\r\n",
        "                idx_change = 0\r\n",
        "\r\n",
        "            update_counter += 1\r\n",
        "            if frames_counter < k2:\r\n",
        "                # Assume k2 = 20,\r\n",
        "                # we don't want to lose all the first frames so we calculating each iteration new background.\r\n",
        "\r\n",
        "                res = np.median(frames[0: frames_counter, :, :, :], axis=0)\r\n",
        "\r\n",
        "                lower_bound_bg = (1 - threshold) * res\r\n",
        "                higher_bound_bg = (1 + threshold) * res\r\n",
        "\r\n",
        "            if update_counter == k2:\r\n",
        "                # If true restart the counting and update the background.\r\n",
        "\r\n",
        "                res = np.median(frames, axis=0)\r\n",
        "                lower_bound_bg = (1 - threshold) * res\r\n",
        "                higher_bound_bg = (1 + threshold) * res\r\n",
        "                update_counter = 0\r\n",
        "\r\n",
        "            \"\"\"\r\n",
        "            Foreach pixel we checking if each channel is in the background's channel range.\r\n",
        "            If one of the channels is in the range so we mark it as background.\r\n",
        "            \r\n",
        "            It's important to note that we can choose constrain at least 2 but then we'll see a lot of noise.\r\n",
        "            Checked both approaches and chose this one, It is all about trade off in the end.\r\n",
        "            \"\"\"\r\n",
        "\r\n",
        "            and_1 = np.logical_and(frame[:, :, 0] > lower_bound_bg[:, :, 0], frame[:, :, 0] < higher_bound_bg[:, :, 0])\r\n",
        "            and_2 = np.logical_and(frame[:, :, 1] > lower_bound_bg[:, :, 1], frame[:, :, 1] < higher_bound_bg[:, :, 1])\r\n",
        "            and_3 = np.logical_and(frame[:, :, 2] > lower_bound_bg[:, :, 2], frame[:, :, 2] < higher_bound_bg[:, :, 2])\r\n",
        "            r = np.logical_or(np.logical_or(and_1, and_2), and_3)\r\n",
        "\r\n",
        "            # Setting pixels as background\r\n",
        "            frame[r, :] = 0\r\n",
        "\r\n",
        "            v_foreground.append(frame.copy())\r\n",
        "\r\n",
        "            # cv2.imshow('frame', frame)\r\n",
        "            # cv2.waitKey(1)\r\n",
        "        else:\r\n",
        "            break\r\n",
        "\r\n",
        "    cap.release()\r\n",
        "\r\n",
        "    return np.array(v_foreground)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIOHVqpn7o8N"
      },
      "source": [
        "def a_1():\r\n",
        "    videos_files = get_dict_video_files()\r\n",
        "    # for key in videos_files.keys():\r\n",
        "    key = \"SLIDE\"\r\n",
        "    # key = \"OneLeaveShopReenter2cor\"\r\n",
        "\r\n",
        "    v_foreground = median_change_dection(name_file=videos_files[key]['input'], k1=10, k2=10)\r\n",
        "    save_output(v_foreground=v_foreground,\r\n",
        "                out_filename_video=videos_files[key]['output_a_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7UfiRDt8OBf"
      },
      "source": [
        "a_1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQmRCFtw7bVg"
      },
      "source": [
        "**A2. Post Processing for  change detection**\n",
        "\n",
        "**Answer:** Suggest a post processing algorithm for improving  the results of a change detection algorithm (e.g., remove noise or fill gaps). \n",
        "\n",
        "**Code:** implement your algorithm.\n",
        "\n",
        "*Input:*  v_original, v_foreground\\\n",
        "v_original: the original video\\\n",
        "v_foreground: the output of B1\n",
        "\n",
        "*output:* v_PP_foreground\\\n",
        "v_PP_foreground: the result of the post processing on v_foreground.\n",
        "\n",
        "\n",
        "**Note:**\n",
        "1. You may want to generate from v_foreground  a binary mask of the foreground regions.\n",
        "2. You can use dilation or erosion on a the binary mask.\n",
        "3.  You may use additional frames to improve the results, but you do not have to.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsNlJEj17sBN"
      },
      "source": [
        "def improve_foreground(v_original: str, v_foreground: str) -> np.ndarray:\r\n",
        "    \"\"\"\r\n",
        "    :param v_original: The original video.\r\n",
        "    :param v_foreground: The output of B1.\r\n",
        "    :return: The result of the post processing on v_foreground.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cap_org = cv2.VideoCapture(v_original)\r\n",
        "    cap_v_fg = cv2.VideoCapture(v_foreground)\r\n",
        "\r\n",
        "    rows, cols = -1, -1\r\n",
        "\r\n",
        "    v_foreground_improved = []\r\n",
        "\r\n",
        "    frames_counter = 0\r\n",
        "    while cap_org.isOpened() and cap_v_fg.isOpened():\r\n",
        "        ret_org, frame_org = cap_org.read()\r\n",
        "        ret_v_fg, frame_v_fg = cap_v_fg.read()\r\n",
        "\r\n",
        "        if ret_org and ret_v_fg:\r\n",
        "            frames_counter += 1\r\n",
        "\r\n",
        "            \"\"\"\r\n",
        "            I'm assuming that most of the pictures and videos (after our algo A.1)\r\n",
        "            where they have areas of pixels with the color (<20, <20, <20) are probababliy noise. \r\n",
        "            \"\"\"\r\n",
        "\r\n",
        "            con1 = np.logical_and(frame_v_fg[:, :, 0] > 20, frame_v_fg[:, :, 1] > 20)\r\n",
        "            con2 = np.logical_and(con1, frame_v_fg[:, :, 2] > 20)\r\n",
        "\r\n",
        "            arr_thre = np.array(con2, dtype=np.uint8)\r\n",
        "\r\n",
        "            # Getting Connected Components With Stats. Stats here is the key.\r\n",
        "            output = cv2.connectedComponentsWithStats(arr_thre, connectivity=8)\r\n",
        "\r\n",
        "            labels = output[0]\r\n",
        "            comps = output[1]\r\n",
        "            stats = output[2]\r\n",
        "\r\n",
        "            labels_arrange = np.arange(labels)\r\n",
        "            labels_arrange = labels_arrange.reshape((len(labels_arrange), 1))\r\n",
        "\r\n",
        "            # Adding new column to stats matrix.\r\n",
        "            stats = np.append(stats, labels_arrange, axis=1)\r\n",
        "\r\n",
        "            # Column 5 (stats[:, 4]) is the size of each component, if its smaller than 4 then delete it.\r\n",
        "            stats_sorted = stats[np.flip(np.argsort(stats[:, 4]))]\r\n",
        "            stats_sorted = stats_sorted[np.where(stats_sorted[:, 4] > 3)]\r\n",
        "            # Removing the biggest component which is the background (all zeros).\r\n",
        "            stats_sorted = stats_sorted[1:]\r\n",
        "\r\n",
        "            # Restoring with the new column we added above all the areas (comps) that are bigger than 3. (>= 4)\r\n",
        "            img_remove_noise = np.zeros(con2.shape, np.uint8)\r\n",
        "            for label in stats_sorted[:, -1]:\r\n",
        "                img_remove_noise[np.where(comps == label)] = 1\r\n",
        "\r\n",
        "            # After removing a lot of noise let's Dialte!\r\n",
        "            kernel = np.ones((5, 5), np.uint8)\r\n",
        "            no_zero_arr: np.ndarray = cv2.dilate(img_remove_noise, kernel, iterations=2)\r\n",
        "\r\n",
        "            # Restoing all the pixels from the original frame.\r\n",
        "            y_idx, x_idx = no_zero_arr.nonzero()\r\n",
        "\r\n",
        "            improved_frame = np.zeros(frame_v_fg.shape, dtype=np.uint8)\r\n",
        "            improved_frame[y_idx, x_idx] = frame_org[y_idx, x_idx]\r\n",
        "\r\n",
        "            # cv2.imshow('frame', frame_v_fg)\r\n",
        "            # cv2.waitKey(1)\r\n",
        "            # print(frames_counter)\r\n",
        "            v_foreground_improved.append(improved_frame)\r\n",
        "        else:\r\n",
        "            break\r\n",
        "\r\n",
        "    cap_org.release()\r\n",
        "    cap_v_fg.release()\r\n",
        "\r\n",
        "    return np.array(v_foreground_improved)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyokpfgR7uT_"
      },
      "source": [
        "def a_2():\r\n",
        "    videos_files = get_dict_video_files()\r\n",
        "    # for key in videos_files.keys():\r\n",
        "    key = \"SLIDE\"\r\n",
        "    # key = \"OneLeaveShopReenter2cor\"\r\n",
        "\r\n",
        "    v_PP_foreground = improve_foreground(v_original=videos_files[key]['input'],\r\n",
        "                                         v_foreground=videos_files[key]['output_a_1'])\r\n",
        "    save_output(v_foreground=v_PP_foreground,\r\n",
        "                out_filename_video=videos_files[key]['output_a_2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNLW-dVFGgSu"
      },
      "source": [
        "a_2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYADD00k7bVg"
      },
      "source": [
        "**A3. Counting  foreground objects**\n",
        "\n",
        "Write a function that counts the number of foreground objects in the result of A1 or A2. \n",
        "\n",
        "*Input:* v_foreground\\\n",
        "v_foreground: a video which is the output of B1 or B2\n",
        "\n",
        "*Output*: c\\\n",
        "c: a vector with the number of foreground objects in each of the frames\n",
        "\n",
        "**Note:** You can use a function that counts connected components in a binary image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erik6fRB71ll"
      },
      "source": [
        "def count_foreground_objects(v_foreground: str, thre_mean: int) -> np.ndarray:\r\n",
        "    \"\"\"\r\n",
        "    :param v_foreground: A video which is the output of A1 or A2\r\n",
        "    :param thre_mean: Threshold for the estimator, see explanation in the Report.docx\r\n",
        "    :return: A vector with the number of foreground objects in each of the frames\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cap = cv2.VideoCapture(v_foreground)\r\n",
        "\r\n",
        "    comps_sizes_frames = []  # Storing for each frame all components size.\r\n",
        "\r\n",
        "    frames_counter = 0\r\n",
        "    while cap.isOpened():\r\n",
        "        ret, frame = cap.read()\r\n",
        "        if ret:\r\n",
        "            frames_counter += 1\r\n",
        "\r\n",
        "            \"\"\"\r\n",
        "            Same logic goes here as A.2.\r\n",
        "            \"\"\"\r\n",
        "\r\n",
        "            con1 = np.logical_and(frame[:, :, 0] > 20, frame[:, :, 1] > 20)\r\n",
        "            con2 = np.logical_and(con1, frame[:, :, 2] > 20)\r\n",
        "\r\n",
        "            arr_thre = np.array(con2, dtype=np.uint8)\r\n",
        "\r\n",
        "            output = cv2.connectedComponentsWithStats(arr_thre, connectivity=8)\r\n",
        "\r\n",
        "            stats = output[2]\r\n",
        "\r\n",
        "            stats_sorted = stats[np.flip(np.argsort(stats[:, 4]))]\r\n",
        "            stats_sorted = stats_sorted[np.where(stats_sorted[:, 4] > 3)]\r\n",
        "            stats_sorted = stats_sorted[1:]\r\n",
        "\r\n",
        "            if stats_sorted.shape[0] == 0:\r\n",
        "                comps_sizes_frames.append(np.array([]))\r\n",
        "                continue\r\n",
        "\r\n",
        "            comps_sizes_frames.append(stats_sorted[:, 4])\r\n",
        "\r\n",
        "            # print(frames_counter)\r\n",
        "        else:\r\n",
        "            break\r\n",
        "\r\n",
        "    cap.release()\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Next I'll count the objects in a way that similar to a lot of estimators in the field, Average of means.\r\n",
        "    The algorithm down below goes like this:\r\n",
        "        1.\tTake the components size average of each frame.\r\n",
        "        2.\tAfter 30 (k) frames calculate the average of the 30 frames from 1.\r\n",
        "        3.\tFor each frame check how many of the components size are bigger than the  estimator (2.).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    c = []\r\n",
        "    avgs = []\r\n",
        "    comps_sizes = []\r\n",
        "    counter = 0\r\n",
        "    k = 30\r\n",
        "    # thre_mean = 1.5\r\n",
        "    for comp_sizes in comps_sizes_frames:\r\n",
        "        if len(comp_sizes) == 0:\r\n",
        "            avgs.append(0)\r\n",
        "        else:\r\n",
        "            avgs.append(np.mean(comp_sizes))\r\n",
        "\r\n",
        "        comps_sizes.append(comp_sizes)\r\n",
        "\r\n",
        "        counter += 1\r\n",
        "        if counter == k:\r\n",
        "            mean = np.mean(avgs) * thre_mean\r\n",
        "\r\n",
        "            for comp in comps_sizes:\r\n",
        "                c.append((comp > mean).sum())\r\n",
        "\r\n",
        "            counter = 0\r\n",
        "            avgs = []\r\n",
        "            comps_sizes = []\r\n",
        "\r\n",
        "    # If there are 95 frames so avgs should be 5, 95 % (k = 30) = 5\r\n",
        "    if len(avgs) > 0:\r\n",
        "        mean = np.mean(avgs) * thre_mean\r\n",
        "\r\n",
        "        for comp in comps_sizes:\r\n",
        "            c.append((comp > mean).sum())\r\n",
        "\r\n",
        "    return np.array(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CvBhIn473O9"
      },
      "source": [
        "def a_3():\r\n",
        "    videos_files = get_dict_video_files()\r\n",
        "    for key in videos_files.keys():\r\n",
        "        # key = \"SLIDE\"\r\n",
        "        key = 'OneLeaveShopReenter2cor'\r\n",
        "\r\n",
        "        thre_mean = 0.5\r\n",
        "        c = count_foreground_objects(v_foreground=videos_files[key]['output_a_2'], thre_mean=thre_mean)\r\n",
        "        print(\"-\" * 10, key, 'A.2', \"-\" * 10)\r\n",
        "        print('Threshold:', thre_mean, '(multiplying our estimator)')\r\n",
        "        print(\"Objects in the video:\", np.round(np.mean(c), 1))\r\n",
        "        print(\"Frames:\\n\", c)\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfawr8IVBShS"
      },
      "source": [
        "a_3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcLdRufq-0qh"
      },
      "source": [
        "## Section B: Compute Optical Flow (OF) using Lucas-Kanade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYlJca9F-1WI"
      },
      "source": [
        "def gaussian_dx(x, y, sigma):\r\n",
        "    return (-x / (2 * np.pi * sigma ** 4)) * np.exp(-(np.square(x) + np.square(y)) / (2 * sigma ** 2))\r\n",
        "\r\n",
        "\r\n",
        "def gaussian_dy(x, y, sigma):\r\n",
        "    return (-y / (2 * np.pi * sigma ** 4)) * np.exp(-(np.square(x) + np.square(y)) / (2 * sigma ** 2))\r\n",
        "\r\n",
        "\r\n",
        "def calc_mask_size(sigma) -> int:\r\n",
        "    def gaussian(_x, _y, _sigma):\r\n",
        "        return (1 / (2 * np.pi * _sigma ** 2)) * np.exp(-(np.square(_x) + np.square(_y)) / (2 * _sigma ** 2))\r\n",
        "\r\n",
        "    size_mask = 1\r\n",
        "    while True:\r\n",
        "        if (size_mask % 2) == 0:\r\n",
        "            size_mask = size_mask + 1\r\n",
        "\r\n",
        "        ax = np.linspace(-(size_mask - 1) / 2., (size_mask - 1) / 2., size_mask)\r\n",
        "\r\n",
        "        x, y = np.meshgrid(ax, ax)\r\n",
        "\r\n",
        "        if np.sum(gaussian(x, y, sigma)) >= 0.95:\r\n",
        "            return size_mask\r\n",
        "        size_mask += 1\r\n",
        "\r\n",
        "\r\n",
        "def gaussian_derivative_xy(sigma) -> (np.ndarray, np.ndarray):\r\n",
        "    mask_size = calc_mask_size(sigma)\r\n",
        "\r\n",
        "    ax = np.linspace(-(mask_size - 1) / 2., (mask_size - 1) / 2., mask_size)\r\n",
        "\r\n",
        "    x, y = np.meshgrid(ax, ax)\r\n",
        "    g_dx = gaussian_dx(x, y, sigma)\r\n",
        "    g_dy = gaussian_dy(x, y, sigma)\r\n",
        "\r\n",
        "    return g_dx, g_dy"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5baeVLd7bVh"
      },
      "source": [
        "\n",
        "\n",
        "**B1. Basic Lucas Kanade OF**\n",
        "\n",
        "Impelment the basic Lucas-Kanade we leared in class.\n",
        "\n",
        "*Input:*  name_file, nf1, nf2,  sigma_R, sigma_S\\\n",
        "name_file: a name of a video file\\\n",
        "f1 and f2: the numbers of the two frames form the video on which the OF is computed.\\\n",
        "sigma_S: the variance of the Gaussian used for the  spatial smoothing  as in HW1 (for computing the derivative of a Gaussian).\\\n",
        "sigma_R: the variance of the Gaussian for computing the sum of the derivatives (the convolution replace the sum).\n",
        "\n",
        "*Output:* U, V, im1,im2\\\n",
        "U, V:  two matrices with the x and y motion for each pixel, respectively.\\\n",
        "im1, im2: the frames on which the optical flow was computed (their number in the video is nf1 and nf2).\n",
        "\n",
        "**Note:**\n",
        "1. You can use any video reading method you find convenient.\n",
        "2. Do not forget to convert the images into grey scale.\n",
        "3. You can compute the derivatives of the images as in HW1 - convolution with the derivative of a Gaussian.\n",
        "3. You can resize the images in order for the program to run faster.\n",
        "5. The computed OF is not necessarily integers. You may want to perform float computation.\n",
        "6. For sigma_R look at slide 63 of Class 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7CQ8SmH41T"
      },
      "source": [
        "def basic_LK_OF(name_file: str, nf1: int, nf2: int, sigma_S: float, sigma_R: float, video_name: str) -> (\r\n",
        "        np.ndarray, np.ndarray, np.ndarray, np.ndarray):\r\n",
        "    \"\"\"\r\n",
        "    :param name_file: A name of a video file.\r\n",
        "    :param nf1: Frame number in the video.\r\n",
        "    :param nf2: Frame number in the video.\r\n",
        "    :param sigma_S: The variance of the Gaussian used for the spatial smoothing.\r\n",
        "    :param sigma_R: The variance of the Gaussian for computing the sum of the derivatives\r\n",
        "    :param video_name: Video name (SLIDE, OneLeaveShopReenter2cor ect)\r\n",
        "    :return:\r\n",
        "    U, V: two matrices with the x and y motion for each pixel, respectively.\r\n",
        "    im1, im2: the frames on which the optical flow was computed (their number in the video is nf1 and nf2).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cap = cv2.VideoCapture(name_file)\r\n",
        "\r\n",
        "    tmp_1 = min(nf1, nf2)\r\n",
        "    tmp_2 = max(nf1, nf2)\r\n",
        "\r\n",
        "    nf1 = tmp_1\r\n",
        "    nf2 = tmp_2\r\n",
        "\r\n",
        "    frame_1 = None\r\n",
        "    frame_1_rgb = None\r\n",
        "    frame_2 = None\r\n",
        "    frame_2_rgb = None\r\n",
        "\r\n",
        "    rows, cols = None, None\r\n",
        "\r\n",
        "    # Getting the frames from the video.\r\n",
        "    frames_counter = 0\r\n",
        "    while cap.isOpened():\r\n",
        "        ret, frame = cap.read()\r\n",
        "        if ret:\r\n",
        "            if frames_counter == nf1:\r\n",
        "                frame_1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n",
        "                frame_1_rgb = frame\r\n",
        "\r\n",
        "            if frames_counter == nf2:\r\n",
        "                frame_2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n",
        "                frame_2_rgb = frame\r\n",
        "\r\n",
        "            if frame_1 is not None and frame_2 is not None:\r\n",
        "                break\r\n",
        "\r\n",
        "            frames_counter += 1\r\n",
        "\r\n",
        "        else:\r\n",
        "            break\r\n",
        "\r\n",
        "    cap.release()\r\n",
        "\r\n",
        "    if frame_1 is None or frame_2 is None:\r\n",
        "        return None, None, None, None\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    For each video I'm using different window size, not because of the content of the video, because its dimensions.\r\n",
        "    camera: 30\r\n",
        "    slide: 50\r\n",
        "    \"\"\"\r\n",
        "    w_size = 30\r\n",
        "    w = w_size // 2\r\n",
        "\r\n",
        "     # Calculating gaussians & using them.\r\n",
        "    g_dx_s, g_dy_s = gaussian_derivative_xy(sigma_S)\r\n",
        "    g_dx_r, g_dy_r = gaussian_derivative_xy(sigma_R)\r\n",
        "\r\n",
        "    mode = 'same'\r\n",
        "\r\n",
        "    f1_i_x = convolve2d(frame_1, g_dx_s, mode=mode)\r\n",
        "    f1_i_y = convolve2d(frame_1, g_dy_s, mode=mode)\r\n",
        "\r\n",
        "    f2_i_x = convolve2d(frame_2, g_dx_s, mode=mode)\r\n",
        "    f2_i_y = convolve2d(frame_2, g_dy_s, mode=mode)\r\n",
        "\r\n",
        "    ft = gaussian_filter(frame_1, sigma_S) - gaussian_filter(frame_2, sigma_S)\r\n",
        "\r\n",
        "    # Preparation for C matrix.\r\n",
        "    f1_i_x_pow = convolve2d(np.power(f1_i_x, 2), g_dx_r, mode=mode)\r\n",
        "    f1_i_y_pow = convolve2d(np.power(f1_i_y, 2), g_dy_r, mode=mode)\r\n",
        "    f1_i_x_i_y = convolve2d(f1_i_x * f1_i_y, g_dx_r, mode=mode)\r\n",
        "\r\n",
        "    # Initializing\r\n",
        "    u, v = np.zeros(frame_1.shape), np.zeros(frame_1.shape)\r\n",
        "    _rows, _cols = frame_1.shape\r\n",
        "\r\n",
        "    for i in range(w, _rows - w):\r\n",
        "        print(f'{i}/{_rows - w}')\r\n",
        "        for j in range(w, _cols - w):\r\n",
        "            # Building C matrix.\r\n",
        "            c = np.array([[f1_i_x_pow[i, j], f1_i_x_i_y[i, j]],\r\n",
        "                          [f1_i_x_i_y[i, j], f1_i_y_pow[i, j]]])\r\n",
        "\r\n",
        "            if np.linalg.matrix_rank(c) != 2:\r\n",
        "                continue\r\n",
        "\r\n",
        "            # Creating A and b matrices\r\n",
        "            i_x = f1_i_x[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "            i_y = f1_i_y[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "            i_t = ft[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "\r\n",
        "            b = np.reshape(i_t, (i_t.shape[0], 1))\r\n",
        "            a = np.vstack((i_x, i_y)).T\r\n",
        "\r\n",
        "            a_pinv = np.linalg.pinv(a)\r\n",
        "\r\n",
        "            uv = np.matmul(a_pinv, b)\r\n",
        "\r\n",
        "            u[i, j] = uv[0]\r\n",
        "            v[i, j] = uv[1]\r\n",
        "\r\n",
        "    return u, v, frame_1_rgb, frame_2_rgb"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lWa4xz5JIZL"
      },
      "source": [
        "videos_files = get_dict_video_files()\r\n",
        "key = \"SLIDE\"\r\n",
        "U_SLIDE, V_SLIDE, im1_SLIDE, im2_SLIDE = basic_LK_OF(name_file=videos_files[key]['input'],\r\n",
        "                                                     nf1=66,\r\n",
        "                                                     nf2=68,\r\n",
        "                                                     sigma_R=1,\r\n",
        "                                                     sigma_S=1,\r\n",
        "                                                     video_name=key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo0DE_vSBPih"
      },
      "source": [
        "videos_files = get_dict_video_files()\r\n",
        "key = \"OneLeaveShopReenter2cor\"\r\n",
        "U_SHOP, V_SHOP, im1_SHOP, im2_SHOP = basic_LK_OF(name_file=videos_files[key]['input'],\r\n",
        "                                                 nf1=80,\r\n",
        "                                                 nf2=84,\r\n",
        "                                                 sigma_R=1,\r\n",
        "                                                 sigma_S=1,\r\n",
        "                                                 video_name=key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "XD6F7u0Y7bVh"
      },
      "source": [
        "**B2. Present OF results**\n",
        "\n",
        "*Input:* im1, U, V (the output of Basic_LK_OF without im2).\n",
        "\n",
        "*Output:* a quiver plot overlaid the input  frame\n",
        "\n",
        "**Note:**\n",
        "1. You can look at https://pythonforundergradengineers.com/quiver-plot-with-matplotlib-and-jupyter-notebooks.html\n",
        "2. You may want for visualaization to uniformally resize the values of U and V - if they are too large or too small/\n",
        "2. You may not want to draw the OF  each pixel - to avoid OF overlapping of neighboring pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuKDTT1vHyYk"
      },
      "source": [
        "def OF_plot_results(U: np.ndarray, V: np.ndarray, im1: np.ndarray):\r\n",
        "    \"\"\"\r\n",
        "     U, V: two matrices with the x and y motion for each pixel, respectively.\r\n",
        "    :param U: Matrix with the x motion for each pixel.\r\n",
        "    :param V: Matrix with the y motion for each pixel.\r\n",
        "    :param im1: Frame (nf1), output from B.1.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    fig.set_size_inches(20, 15)\r\n",
        "\r\n",
        "    # Converting from BRG (cv2) to RGB.\r\n",
        "    ax.imshow(cv2.cvtColor(im1, cv2.COLOR_BGR2RGB))\r\n",
        "\r\n",
        "    _rows, _cols, _ = im1.shape\r\n",
        "\r\n",
        "    x_pos = np.array([np.arange(_cols)])\r\n",
        "    x_pos = np.repeat(x_pos, _rows, axis=0)\r\n",
        "\r\n",
        "    y_pos = np.arange(_rows).reshape((_rows, 1))\r\n",
        "    y_pos = np.repeat(y_pos, _cols, axis=1)\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    I want to show in pretty way all the arrows so I found the right balace.\r\n",
        "    Can be extended to formula according to the dimensions of the video.\r\n",
        "    camera: 200\r\n",
        "    slide: 400\r\n",
        "    \"\"\"\r\n",
        "    x_direct = U * 400\r\n",
        "    y_direct = V * 400\r\n",
        "\r\n",
        "    strength = np.sqrt(np.square(x_direct) + np.square(y_direct))\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Showing every i arrow in x axis and j arrow in y axis.\r\n",
        "    camera:   rows: 10    cols: 5\r\n",
        "    slide:    rows: 15    cols: 30\r\n",
        "    \"\"\"\r\n",
        "    set_to_zero_rows = np.arange(_rows).reshape((_rows, 1))\r\n",
        "    set_to_zero_rows = np.repeat(set_to_zero_rows, _cols, axis=1)\r\n",
        "    set_to_zero_rows = np.where(set_to_zero_rows % 15 != 0)\r\n",
        "    strength[set_to_zero_rows] = 0\r\n",
        "\r\n",
        "    set_to_zero_cols = np.arange(_cols).reshape((1, _cols))\r\n",
        "    set_to_zero_cols = np.repeat(set_to_zero_cols, _rows, axis=0)\r\n",
        "    set_to_zero_cols = np.where(set_to_zero_cols % 30 != 0)\r\n",
        "    strength[set_to_zero_cols] = 0\r\n",
        "\r\n",
        "    st = strength[strength.nonzero()]\r\n",
        "\r\n",
        "    # Taking indices of all the arrows with a change.\r\n",
        "    threshold = np.mean(st)\r\n",
        "    indices = np.where(np.abs(strength) > 0)\r\n",
        "\r\n",
        "    # Taking only arrows that bigger than the average.\r\n",
        "    valid_arrows_count = len(np.where(np.abs(strength) > threshold)[0])\r\n",
        "\r\n",
        "    strength = strength[indices]\r\n",
        "\r\n",
        "    x_direct = x_direct[indices]\r\n",
        "    y_direct = y_direct[indices]\r\n",
        "\r\n",
        "    x_pos = x_pos[indices]\r\n",
        "    y_pos = y_pos[indices]\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Next is something custom I made for better understanding which arrow is strong and which is not.\r\n",
        "    The strong arrows will have high alpha (color transparency) and weak low alpha.   \r\n",
        "    \"\"\"\r\n",
        "    c = np.array([colors.to_rgba([0, 0, 0], alpha=1)])\r\n",
        "\r\n",
        "    _colors = np.repeat(c, len(x_pos), axis=0)\r\n",
        "\r\n",
        "    _sorted = np.flip(np.argsort(strength))\r\n",
        "\r\n",
        "    step = 0.8 / valid_arrows_count\r\n",
        "    alpha = 1.0\r\n",
        "    counter = 0\r\n",
        "    # step = 0\r\n",
        "    for arrow in _sorted:\r\n",
        "        if counter > valid_arrows_count:\r\n",
        "            _colors[arrow][3] = 0\r\n",
        "        else:\r\n",
        "            _colors[arrow][3] = alpha\r\n",
        "            alpha -= step\r\n",
        "\r\n",
        "        counter += 1\r\n",
        "\r\n",
        "    # units=\"width/xy\"\r\n",
        "    # ax.quiver(x_pos, y_pos, x_direct, y_direct, scale=10, color=_colors, units=\"xy\")\r\n",
        "    ax.quiver(x_pos, y_pos, x_direct, y_direct, scale=10, color=_colors, units=\"xy\")\r\n",
        "\r\n",
        "    ax.axis([0, _cols, 0, _rows])\r\n",
        "    ax.set_aspect('equal')\r\n",
        "\r\n",
        "    plt.gca().invert_yaxis()\r\n",
        "    plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMIw4cpCL5O7"
      },
      "source": [
        "OF_plot_results(U=U_SLIDE, V=-V_SLIDE, im1=im1_SLIDE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28wnxAr3NyxK"
      },
      "source": [
        "OF_plot_results(U=U_SHOP, V=-V_SHOP, im1=im1_SHOP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HGZ0ZW-7bVi"
      },
      "source": [
        "**B3. Evaluate OF results**\n",
        "\n",
        "*Input:* im1, im2,  U, V (the output of Basic_LK_OF).\n",
        "\n",
        "*Output:* w_im1, w_diff, err\\\n",
        "w_im1: the results of wrapping im1 using (U,V) toward im2 (matrix).\\\n",
        "w_diff:  |wraped_im1 -  im2| (matrix).\\\n",
        "err: the sum of square differences between w_im1 and im2 (scalar).\n",
        "\n",
        "**Note:**\\\n",
        " You can use any wrapping function you like from openCV or other code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVwhL8xewsal"
      },
      "source": [
        "def evaluate_OF_results(im1: np.ndarray, im2: np.ndarray, U: np.ndarray, V: np.ndarray) -> (\r\n",
        "        np.ndarray, np.ndarray, float):\r\n",
        "    im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)\r\n",
        "    im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)\r\n",
        "\r\n",
        "    x_direct_pixels = np.array(np.round(U * 20, decimals=0), dtype=np.int)\r\n",
        "    y_direct_pixels = np.array(np.round(V * 20, decimals=0), dtype=np.int)\r\n",
        "\r\n",
        "    im1_to_im2 = im1.copy()\r\n",
        "\r\n",
        "    # print(np.mean(U))\r\n",
        "    # print(np.mean(V))\r\n",
        "\r\n",
        "    U_sorted = np.sort(np.sqrt(np.square(U) + np.square(V)), axis=1)\r\n",
        "\r\n",
        "    strength = np.sqrt(np.square(U) + np.square(V))\r\n",
        "\r\n",
        "    y_idx, x_idx = np.where(strength > 0.1)\r\n",
        "\r\n",
        "    for i in range(y_idx.shape[0]):\r\n",
        "        y = y_idx[i]\r\n",
        "        x = x_idx[i]\r\n",
        "\r\n",
        "        change_x = x_direct_pixels[y, x]\r\n",
        "        change_y = y_direct_pixels[y, x]\r\n",
        "\r\n",
        "        im1_to_im2[y + change_y, x + change_x] = im1[y, x]\r\n",
        "\r\n",
        "    w_diff = np.abs(im1_to_im2 - im2)\r\n",
        "    err = np.sum(np.power(np.array(im1_to_im2, dtype=np.float32) - np.array(im2, dtype=np.float32), 2))\r\n",
        "    # plt.imshow(w_diff)\r\n",
        "    # plt.imshow(im1_to_im2)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "    return im1_to_im2, w_diff, err"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NAQFePc1_au"
      },
      "source": [
        "im1_to_im2_SLIDE, w_diff_SLIDE, error_SLIDE = evaluate_OF_results(im1=im1_SLIDE, im2=im2_SLIDE, U=U_SLIDE, V=V_SLIDE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjiyuhY0DT8g",
        "outputId": "d2d8aedd-4af7-469b-c7f9-3b6381d7a4cf"
      },
      "source": [
        "error_SLIDE"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "892727360.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_96PY59OC5gu"
      },
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "fig.set_size_inches(20, 15)\r\n",
        "\r\n",
        "ax.imshow(im1_to_im2_SLIDE)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p1qAl77DHuU"
      },
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "fig.set_size_inches(20, 15)\r\n",
        "\r\n",
        "ax.imshow(w_diff_SLIDE)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuN82337DYkO"
      },
      "source": [
        "im1_to_im2_SHOP, w_diff_SHOP, error_SHOP = evaluate_OF_results(im1=im1_SHOP, im2=im2_SHOP, U=U_SHOP, V=V_SHOP)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnhgrpBuDpNx",
        "outputId": "9977ac8d-0bd4-49c8-9800-106b83a6a363"
      },
      "source": [
        "error_SHOP"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91407030.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVJ3J_9NDnss"
      },
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "fig.set_size_inches(20, 15)\r\n",
        "\r\n",
        "ax.imshow(im1_to_im2_SHOP)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JExa8UUDDqPo"
      },
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "fig.set_size_inches(20, 15)\r\n",
        "\r\n",
        "ax.imshow(w_diff_SHOP)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-5-6NFO7bVi"
      },
      "source": [
        "**B4. Affine_LK_OF**\n",
        "\n",
        "Use the variant of Lucas-Kanade with affine motion instead of translation.\n",
        "See slides - class 7 slides 73-75.\n",
        "\n",
        "The input and output is the same as in **B1**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B20jkR-AEs35"
      },
      "source": [
        "def affine_LK_OF(name_file: str, nf1: int, nf2: int, sigma_S: float, sigma_R: float, video_name: str) -> (\r\n",
        "        np.ndarray, np.ndarray, np.ndarray, np.ndarray):\r\n",
        "    \"\"\"\r\n",
        "    :param name_file: A name of a video file.\r\n",
        "    :param nf1: Frame number in the video.\r\n",
        "    :param nf2: Frame number in the video.\r\n",
        "    :param sigma_S: The variance of the Gaussian used for the spatial smoothing.\r\n",
        "    :param sigma_R: The variance of the Gaussian for computing the sum of the derivatives\r\n",
        "    :param video_name: Video name (SLIDE, OneLeaveShopReenter2cor ect)\r\n",
        "    :return:\r\n",
        "    U, V: two matrices with the x and y motion for each pixel, respectively.\r\n",
        "    im1, im2: the frames on which the optical flow was computed (their number in the video is nf1 and nf2).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    cap = cv2.VideoCapture(name_file)\r\n",
        "\r\n",
        "    tmp_1 = min(nf1, nf2)\r\n",
        "    tmp_2 = max(nf1, nf2)\r\n",
        "\r\n",
        "    nf1 = tmp_1\r\n",
        "    nf2 = tmp_2\r\n",
        "\r\n",
        "    frame_1 = None\r\n",
        "    frame_1_rgb = None\r\n",
        "    frame_2 = None\r\n",
        "    frame_2_rgb = None\r\n",
        "\r\n",
        "    rows, cols = None, None\r\n",
        "\r\n",
        "    # Getting the frames from the video.\r\n",
        "    frames_counter = 0\r\n",
        "    while cap.isOpened():\r\n",
        "        ret, frame = cap.read()\r\n",
        "        if ret:\r\n",
        "            if frames_counter == nf1:\r\n",
        "                frame_1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n",
        "                frame_1_rgb = frame\r\n",
        "\r\n",
        "            if frames_counter == nf2:\r\n",
        "                frame_2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n",
        "                frame_2_rgb = frame\r\n",
        "\r\n",
        "            if frame_1 is not None and frame_2 is not None:\r\n",
        "                break\r\n",
        "\r\n",
        "            frames_counter += 1\r\n",
        "\r\n",
        "        else:\r\n",
        "            break\r\n",
        "\r\n",
        "    cap.release()\r\n",
        "\r\n",
        "    if frame_1 is None or frame_2 is None:\r\n",
        "        return None, None\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    For each video I'm using different window size, not because of the content of the video, because its dimensions.\r\n",
        "    camera: 30\r\n",
        "    slide: 50\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    w_size = 50\r\n",
        "    w = w_size // 2\r\n",
        "\r\n",
        "    # Calculating gaussians & using them.\r\n",
        "    g_dx_s, g_dy_s = gaussian_derivative_xy(sigma_S)\r\n",
        "    g_dx_r, g_dy_r = gaussian_derivative_xy(sigma_R)\r\n",
        "\r\n",
        "    mode = 'same'\r\n",
        "\r\n",
        "    f1_i_x = convolve2d(frame_1, g_dx_s, mode=mode)\r\n",
        "    f1_i_y = convolve2d(frame_1, g_dy_s, mode=mode)\r\n",
        "\r\n",
        "    f2_i_x = convolve2d(frame_2, g_dx_s, mode=mode)\r\n",
        "    f2_i_y = convolve2d(frame_2, g_dy_s, mode=mode)\r\n",
        "\r\n",
        "    ft = gaussian_filter(frame_1, sigma_S) - gaussian_filter(frame_2, sigma_S)\r\n",
        "\r\n",
        "    # Preparation for C matrix.\r\n",
        "    f1_i_x_pow = convolve2d(np.power(f1_i_x, 2), g_dx_r, mode=mode)\r\n",
        "    f1_i_y_pow = convolve2d(np.power(f1_i_y, 2), g_dy_r, mode=mode)\r\n",
        "    f1_i_x_i_y = convolve2d(f1_i_x * f1_i_y, g_dx_r, mode=mode)\r\n",
        "\r\n",
        "    # Initializing\r\n",
        "    u = np.zeros(frame_1.shape)\r\n",
        "    v = u.copy()\r\n",
        "\r\n",
        "    _rows, _cols = frame_1.shape\r\n",
        "\r\n",
        "    xu, yv = np.meshgrid(np.arange(0, _cols), np.arange(0, _rows))\r\n",
        "\r\n",
        "    for i in range(w, _rows - w):\r\n",
        "        print(f'{i}/{_rows - w}')\r\n",
        "        for j in range(w, _cols - w):\r\n",
        "            i_x = f1_i_x[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "            i_y = f1_i_y[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "            i_t = ft[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "            _x = xu[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "            _y = yv[i - w: i + w + 1, j - w: j + w + 1].flatten()\r\n",
        "\r\n",
        "            b = np.reshape(i_t, (i_t.shape[0], 1))\r\n",
        "            a = np.vstack((i_x, _x * i_x, _y * i_x, i_y, _x * i_y, _y * i_y)).T\r\n",
        "            c = np.matmul(np.transpose(a), a)\r\n",
        "\r\n",
        "            if np.linalg.matrix_rank(c) != 6:\r\n",
        "                continue\r\n",
        "\r\n",
        "            a_pinv = np.linalg.pinv(a)\r\n",
        "\r\n",
        "            uv = np.matmul(a_pinv, b)\r\n",
        "\r\n",
        "            u[i, j] = uv[0] + uv[1] * j + uv[2] * i\r\n",
        "            v[i, j] = uv[3] + uv[4] * j + uv[5] * i\r\n",
        "\r\n",
        "    return u, v, frame_1_rgb, frame_2_rgb"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NONl94CoFEha"
      },
      "source": [
        "videos_files = get_dict_video_files()\r\n",
        "key = \"SLIDE\"\r\n",
        "affine_U_SLIDE, affine_V_SLIDE, affine_im1_SLIDE, affine_im2_SLIDE = affine_LK_OF(name_file=videos_files[key]['input'],\r\n",
        "                                                                                  nf1=66,\r\n",
        "                                                                                  nf2=68,\r\n",
        "                                                                                  sigma_R=1,\r\n",
        "                                                                                  sigma_S=1,\r\n",
        "                                                                                  video_name=key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPNLRfRBFiMF"
      },
      "source": [
        "videos_files = get_dict_video_files()\r\n",
        "key = \"OneLeaveShopReenter2cor\"\r\n",
        "affine_U_SHOP, affine_V_SHOP, affine_im1_SHOP, affine_im2_SHOP = affine_LK_OF(name_file=videos_files[key]['input'],\r\n",
        "                                                                              nf1=80,\r\n",
        "                                                                              nf2=84,\r\n",
        "                                                                              sigma_R=1,\r\n",
        "                                                                              sigma_S=1,\r\n",
        "                                                                              video_name=key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kmb7kCsI4va"
      },
      "source": [
        "OF_plot_results(U=affine_U_SLIDE, V=-affine_V_SLIDE, im1=affine_im1_SLIDE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "covswW_h7bVi"
      },
      "source": [
        "**B5. Apply and Discuss:**\n",
        "\n",
        "Run the basic_LK_OF, and the affine_LK_OF on one or two videos, one with a static camera and the other with a moving camera.\n",
        "Play with the frames chosen from each video, the algorithm parameters, and  the distance between nf1 and nf2.\n",
        "\n",
        "**Answer:**\n",
        "1. The disparity you compute in HW2 were integers while the OF is not necessarily integer. Expalin why. \n",
        "2. Explain theoretically for which regions the basic_LK_OF is expected to give good results and for which it does not.\n",
        "2. Demonstrate your answer to (2) by displaying the results of OF overlaid im1  (Quiver overlayed im1), and mark good and bad results.\n",
        "3. Explain theoretically when the basic_LK_OF is expected to fail while affine_LK_OF works well.\n",
        "4. Find an example for (4)  (at least a region in the scene) and display it.\n",
        "5. When two OF vectors have the same magnitude, are they necessarily corresponds to 3D points that moves at the same speed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oi5S7DE7bVj"
      },
      "source": [
        "## Section C: Segementation  -- Not for submission\n",
        "\n",
        "Part C will not be graded, hence,  **you do not have to submit it**. In case you would like to get feedback on it anyway - you are welcome to submit it.\n",
        "\n",
        "**C1. Simple OF segemntation**\n",
        "\n",
        "Use a simple segmentation (e.g., threshold) for the OF results based on the OF magnitude.\n",
        "\n",
        "*Input:* U, V, any other parameters you find necessary\n",
        "U, V - the OF vectors (e.g., computed by  B1 or B4).\n",
        "\n",
        "*Output:* segments\n",
        "im_segments is an image in which each segment is colored by a different color.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9mAJeR87bVj"
      },
      "source": [
        "def im_segments = simple_segment_OF(U,V, \"any other paramers\")\n",
        "   \n",
        "    # Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypM4amIH7bVk"
      },
      "source": [
        "**C2. K-means OF segemntation**\n",
        "\n",
        "Use k-mean segmentation for the OF results based on the OF magnitude and directions.\n",
        "\n",
        "*Input:* U, V, any other parameters you find necessary\\\n",
        "U, V - the OF vectors (e.g., computed by  B1 or B4).\n",
        "\n",
        "*Output:* im_segments\\\n",
        "im_segments is an image in which each segment is colored by a different color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyO2Gc0L7bVk"
      },
      "source": [
        "def im_segments = simple_segment_OF(U,V, \"any other paramers\")\n",
        "   \n",
        "    # Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5jRU_U57bVk"
      },
      "source": [
        "**C3. Apply  and answer**\n",
        "\n",
        "1. Apply the functions in C1 and C2 on a video of your choice with a moving camera.\n",
        "2. Discuss which method (C1 or C2) works better. Give an example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yxUWl7I7bVk"
      },
      "source": [
        "**C4. OF used for change detection**\n",
        "\n",
        "Assume that the video is taken by a static camera and use the results of C1 or C2 on the output in order to detect moving regions in the scene.\n",
        "\n",
        "*input:* name_file, nf1, \"any parameters you need\"\\\n",
        "name_file: a name of a video file.\\\n",
        "nf1: the frame on which the OF is computed (nf1 and nf1+1).\n",
        "\n",
        "*output*: v_change\\\n",
        "v_foreground: as in A1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxyofbFB7bVk"
      },
      "source": [
        "def [v_foreground] = OF_change_detection(name_file, nf1, \"any parameters you need\"):\n",
        "    \n",
        "    # your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5a4rQia7bVk"
      },
      "source": [
        "**C5. Change detection comparisons**\n",
        "\n",
        "Choose a video taken from a static camera (one of the videos you used for change detection). \n",
        "\n",
        "1. Apply the functions A1 or A2 and C4.\n",
        "4. Discuss which method (A1 or C4) works better. Give an example.\n",
        "5. Count the number of moving regions for A1, A3, and C4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySiqzRxV7bVl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}